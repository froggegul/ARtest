<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AR Model Viewer — Face Control (v6)</title>
  <!-- Import map to resolve Three.js module specifiers -->
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.150.0/build/three.module.js",
        "three/examples/jsm/": "https://cdn.jsdelivr.net/npm/three@0.150.0/examples/jsm/"
      }
    }
  </script>
  <!-- MediaPipe Face Mesh and drawing utils -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" defer></script>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      height: 100%;
      background: #000;
      font-family: sans-serif;
      overflow: hidden;
    }
    #ar-container {
      position: relative;
      width: 100vw;
      height: 100vh;
      overflow: hidden;
    }
    #videoElement {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
      z-index: 1;
    }
    /* Place overlayCanvas between video and 3D model so it doesn't cover the 3D object */
    #overlayCanvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: 2;
      pointer-events: none;
    }
    #threeCanvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: 3;
      pointer-events: none;
    }
    #controls {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 4;
      background: rgba(0,0,0,0.5);
      padding: 8px;
      border-radius: 4px;
      color: #fff;
    }
    #info {
      margin-top: 4px;
      font-size: 14px;
    }
  </style>
</head>
<body>
  <div id="ar-container">
    <video id="videoElement" playsinline muted></video>
    <canvas id="overlayCanvas"></canvas>
    <canvas id="threeCanvas"></canvas>
    <div id="controls">
      <div id="info">얼굴을 감지하면 자동으로 모델을 로드합니다.</div>
    </div>
  </div>
  <script type="module">
    import * as THREE from 'three';
    import { OBJLoader } from 'three/examples/jsm/loaders/OBJLoader.js';

    const videoElement = document.getElementById('videoElement');
    const threeCanvas = document.getElementById('threeCanvas');
    const overlayCanvas = document.getElementById('overlayCanvas');
    const infoEl = document.getElementById('info');
    const overlayCtx = overlayCanvas.getContext('2d');

    // Three.js setup
    const renderer = new THREE.WebGLRenderer({ canvas: threeCanvas, alpha: true, antialias: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio);
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 0.1, 1000);
    camera.position.z = 3;
    scene.add(new THREE.AmbientLight(0xffffff, 0.6));
    const dirLight = new THREE.DirectionalLight(0xffffff, 0.8);
    dirLight.position.set(1, 1, 2);
    scene.add(dirLight);
    const modelGroup = new THREE.Group();
    scene.add(modelGroup);
    let currentModel = null;
    let targetRotY = 0;
    const FIXED_ROT_X = 0;
    const FIXED_ROT_Z = 0;
    let targetScale = 1;
    let prevYaw = null;
    // Variables to handle relative scaling based on initial mouth close event
    let scalingActive = false;
    let referenceFaceWidth = null;
    let referenceTargetScale = null;
    // Rotation sensitivity multiplier (1.3 = 30% increase) and inversion
    const ROTATION_SCALE = 1.3;

    function animate() {
      requestAnimationFrame(animate);
      modelGroup.rotation.x = FIXED_ROT_X;
      modelGroup.rotation.z = FIXED_ROT_Z;
      modelGroup.rotation.y += (targetRotY - modelGroup.rotation.y) * 0.2;
      const curScale = modelGroup.scale.x;
      modelGroup.scale.setScalar(curScale + (targetScale - curScale) * 0.2);
      renderer.render(scene, camera);
    }
    animate();

    function handleResize() {
      const w = window.innerWidth;
      const h = window.innerHeight;
      renderer.setSize(w, h);
      overlayCanvas.width = w;
      overlayCanvas.height = h;
      camera.aspect = w / h;
      camera.updateProjectionMatrix();
    }
    window.addEventListener('resize', handleResize);
    handleResize();

    async function loadDefaultModel() {
      try {
        const objUrl = 'tripo_convert_82cea090-6813-43f2-a343-2a173d03f37d.obj';
        const baseTexUrl = 'tripo_image_82cea090-6813-43f2-a343-2a173d03f37d_0.jpg';
        const normalTexUrl = 'tripo_image_82cea090-6813-43f2-a343-2a173d03f37d_2.jpg';
        const metalTexUrl = 'tripo_image_82cea090-6813-43f2-a343-2a173d03f37d_Metallic.jpg';
        const roughTexUrl = 'tripo_image_82cea090-6813-43f2-a343-2a173d03f37d_Roughness.jpg';
        const objResponse = await fetch(objUrl);
        const objText = await objResponse.text();
        const loader = new OBJLoader();
        const object = loader.parse(objText);
        const textureLoader = new THREE.TextureLoader();
        const [baseTex, normalTex, metalTex, roughTex] = await Promise.all([
          textureLoader.loadAsync(baseTexUrl),
          textureLoader.loadAsync(normalTexUrl),
          textureLoader.loadAsync(metalTexUrl),
          textureLoader.loadAsync(roughTexUrl)
        ]);
        [baseTex, normalTex, metalTex, roughTex].forEach(tex => {
          tex.wrapS = tex.wrapT = THREE.RepeatWrapping;
          tex.encoding = THREE.sRGBEncoding;
        });
        object.traverse((child) => {
          if (child.isMesh) {
            const mat = new THREE.MeshStandardMaterial({ color: 0xffffff });
            mat.map = baseTex;
            mat.normalMap = normalTex;
            mat.metalnessMap = metalTex;
            mat.metalness = 1.0;
            mat.roughnessMap = roughTex;
            mat.roughness = 1.0;
            mat.needsUpdate = true;
            child.material = mat;
          }
        });
        const box = new THREE.Box3().setFromObject(object);
        const size = new THREE.Vector3();
        box.getSize(size);
        const maxDim = Math.max(size.x, size.y, size.z);
        const scaleFactor = maxDim > 0 ? 1.5 / maxDim : 1.0;
        object.scale.setScalar(scaleFactor);
        const center = new THREE.Vector3();
        box.getCenter(center);
        center.multiplyScalar(scaleFactor);
        object.position.set(-center.x, -center.y, -center.z);
        object.rotation.y = Math.PI;
        if (currentModel) modelGroup.remove(currentModel);
        currentModel = object;
        modelGroup.add(object);
      } catch (err) {
        console.error(err);
        infoEl.textContent = '모델 로드 실패';
      }
    }

    function computeFaceYaw(landmarks) {
      const left = landmarks[234];
      const right = landmarks[454];
      return Math.atan2(left.z - right.z, right.x - left.x);
    }
    function computeFaceWidth(landmarks) {
      const left = landmarks[234];
      const right = landmarks[454];
      const dx = right.x - left.x;
      const dy = right.y - left.y;
      return Math.hypot(dx, dy);
    }
    function isMouthClosed(landmarks) {
      const upper = landmarks[13];
      const lower = landmarks[14];
      const dist = Math.hypot(upper.x - lower.x, upper.y - lower.y);
      return dist < 0.03;
    }

    async function setupFaceTracking() {
      if (!window.FaceMesh || !window.drawLandmarks || !window.drawConnectors) {
        console.error('MediaPipe Face Mesh not loaded');
        return;
      }
      const faceMesh = new FaceMesh({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}` });
      faceMesh.setOptions({ maxNumFaces: 1, refineLandmarks: true, minDetectionConfidence: 0.5, minTrackingConfidence: 0.5 });
      faceMesh.onResults((results) => {
        overlayCtx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
          const landmarks = results.multiFaceLandmarks[0];
          // Draw only the lips region with semi-transparent red and skip drawing full face mesh
          overlayCtx.save();
          overlayCtx.globalAlpha = 0.5;
          if (typeof FACEMESH_LIPS !== 'undefined') {
            drawConnectors(overlayCtx, landmarks, FACEMESH_LIPS, { color: '#FF4444', lineWidth: 2 });
          }
          overlayCtx.restore();

          const mouthClosed = isMouthClosed(landmarks);
          if (mouthClosed) {
            // Update rotation: invert direction and scale by ROTATION_SCALE
            const yaw = computeFaceYaw(landmarks);
            if (prevYaw !== null) {
              const deltaYaw = yaw - prevYaw;
              // invert direction: subtract delta and multiply by ROTATION_SCALE to increase sensitivity
              targetRotY -= deltaYaw * ROTATION_SCALE;
            }
            prevYaw = yaw;
            // Handle scaling relative to first closed frame
            const currentWidth = computeFaceWidth(landmarks);
            if (!scalingActive) {
              // Start scaling: store reference width and scale
              scalingActive = true;
              referenceFaceWidth = currentWidth;
              referenceTargetScale = targetScale;
            }
            // Calculate scale relative to reference
            let newScale = referenceTargetScale * (currentWidth / referenceFaceWidth);
            // Clamp scale to reasonable range
            newScale = Math.max(0.4, Math.min(3.0, newScale));
            targetScale = newScale;
            infoEl.textContent = '입을 다문 상태: 회전/크기 조절 중...';
          } else {
            // Mouth open: stop updating but do not reset scale
            scalingActive = false;
            prevYaw = null;
            infoEl.textContent = '입을 벌리면 모델이 고정됩니다.';
          }
        } else {
          scalingActive = false;
          prevYaw = null;
          infoEl.textContent = '얼굴을 카메라에 비춰 주세요.';
        }
      });
      async function processFrame() {
        await faceMesh.send({ image: videoElement });
        requestAnimationFrame(processFrame);
      }
      processFrame();
    }

    async function initCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoElement.srcObject = stream;
        await videoElement.play();
        await loadDefaultModel();
        infoEl.textContent = '얼굴 인식 중...';
        await setupFaceTracking();
      } catch (err) {
        console.error(err);
        infoEl.textContent = '카메라 접근을 허용해야 합니다.';
      }
    }
    window.addEventListener('load', initCamera);
  </script>
</body>
</html>
